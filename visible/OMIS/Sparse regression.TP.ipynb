{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression linéaire\n",
    "===================\n",
    "\n",
    "L'objectif de ce TP est d'effectuer les différents calculs des notions vues en cours sur le modèle\n",
    "$$Y = X\\beta + \\varepsilon$$\n",
    "===========================\n",
    "dans le cas où $X$ est ou non de plein rang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les librairies dont on aura besoin pour le TP, à savoir *numpy* pour les outils mathématiques et *sklearn* pour les outils d'apprentissage.\n",
    "\n",
    "Pour que les *plot* s'affiche dans la page, on ajoute\n",
    "\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Méthode des moindres carrés ordinaire\n",
    " \n",
    " Dans un premier temps, nous allons générer $Y$ en fonction de $X$ à partir d'un $\\beta$ que l'on se fixe.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=50\n",
    "beta=[1,4,1,-7,-2,1,0.7,0.1,3,0.01]\n",
    "X=np.random.randn(n,10)\n",
    "eps=np.random.normal(0,1,n)\n",
    "Y=np.dot(X,beta)+eps\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est de retrouver $Y$ à partir de l'observation de $X$ et $Y$.\n",
    "\n",
    "Dans un premier, on affiche sur un graphe $Y$ en fonction d'une coordonnée de $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y, X[:,8], alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(r'$Y$ en fonction de $X_9\\beta_9$')\n",
    "\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va avoir du mal à prédire $Y$ seulement à partir de $X_9$.\n",
    "\n",
    "**Calculer l'erreur quadratique moyenne dans cette régression est de $Y$ sur $X_9$ lorsque $\\beta_9=3$ est connu.**\n",
    "\n",
    "On pourra utiliser la méthode\n",
    "\n",
    "    np.linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(np.linalg.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Puis l'erreur quadratique moyenne de la régression de $Y$ sur $X$ lorsque $\\beta$ est connu.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme attendu, l'écart quadratique moyen est inférieur lorsque l'on régresse $Y$ sur toute les colonnes de $X$.\n",
    "Autrement dit, en utilisant toute l'information que l'on a sur $Y$ (i.e. toute les colonnes de $X$), on fait mieux.\n",
    "\n",
    "Maintenant, on suppose que $\\beta$ est inconnue. \n",
    "On veut le retrouver à partir de nos observations de $X$ et $Y$.\n",
    "\n",
    "**Estimer $\\beta$ en appliquant la méthode des moindres carrés.**\n",
    "\n",
    "On pourra utiliser les méthodes\n",
    "\n",
    "    np.linalg.inv\n",
    "    np.linalg.dot\n",
    "    np.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(np.linalg.inv)\n",
    "#help(np.linalg.dot)\n",
    "#help(np.transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variance de $\\varepsilon$ est $1$. À partir de la formule du cours, **donner la variance des $\\hat\\beta_i$ (conditionnellement à $X$).**\n",
    "\n",
    "On pourra utiliser \n",
    "\n",
    "        np.diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(np.diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En déduire un intervalle de confiance à 0.95 des $\\beta_j$.\n",
    "Les *vrais* $\\beta_j$ sont-ils dans cet intervalle?**\n",
    "\n",
    "Pour rappel, le quantile d'ordre 0.975 de la loi normale est $\\approx$ 1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas parcimonieux\n",
    "\n",
    "On reprend maintenant le même contexte, mais cette fois, des colonnes sont ajoutées à $X$. Autrement dit, l'information qui nous permet d'estimer $X$ (i.e. les colonnes $j$ de $X$ ayant un grand coefficient $\\beta_j$) est masquée parmi un grand nombre de colonnnes de $X$.\n",
    "On note alors $\\beta'$ ce nouveau vecteur.\n",
    "\n",
    "*Pour les modèles avec parcimonie, on notera $k$ le nombre de coordonnées non nulles de $\\beta$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=7000\n",
    "k=len(beta)\n",
    "betap=np.concatenate((beta,np.zeros(d-k)))\n",
    "np.random.shuffle(betap)\n",
    "print(betap)\n",
    "Xp=np.random.randn(n,d)\n",
    "Yp=np.dot(Xp,betap)+eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que renvoie la méthode des moindres carrés?\n",
    "Commenter**\n",
    "\n",
    "*Enregistrer votre TP avant de lancer le calcul*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir retrouver nos coefficients, on va utiliser la méthode LASSO dans un premier temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode LASSO\n",
    "\n",
    "La méthode de régression LASSO est implémentée dans la librairie *scikitlearn* via la classe\n",
    "\n",
    "    linear_model.Lasso\n",
    "   \n",
    "Nous avons vu qu'il est possible d'ajouter une colonne de $1$ à $X$ pour des données non centrées. Cette classe permet d'automatiser l'ajout de cette colonne, qui est alors appelée *intercept*.\n",
    "Comme on sait que dans notre modèle, il n'y a pas de colonne de 1 et que cette classe l'ajoute par défaut, on va lui demander de ne pas l'ajouter via l'argument *intercept=False*\n",
    "\n",
    "*Dans cette classe, le $\\lambda$ du cours est appelé* alpha\n",
    "\n",
    "On utilise la valeur $\\lambda$ donnée en cours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l=8*np.sqrt(np.log(2*d)/n)\n",
    "clf = linear_model.Lasso(alpha=l,fit_intercept=False)\n",
    "clf.fit(Xp, Yp)\n",
    "hatbetap=clf.coef_\n",
    "\n",
    "print(betap[betap!=0])\n",
    "print(betap[hatbetap!=0])\n",
    "print(hatbetap[betap!=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commenter. Est-ce que la méthode semble bien fonctionner?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification des hypothèses\n",
    "Vérifions si les hypothèses de nos théorèmes sont bien satisfaites.\n",
    "\n",
    "Pour que l'on puisse appliquer le théorème du cours qui affirme que \n",
    "$$\\mathbb{E}\\frac{1}{n}\\|Y-X\\hat\\beta_{LASSO}\\|^2\\lesssim k\\sigma^2\\frac{\\log(2d)}{n},$$\n",
    "où ici $k=len(\\beta)=10$;\n",
    "nous devons vérifier que la matrice $X^tX$ vérifie la condition d'incohérence\n",
    "\n",
    "$$ \\sup \\left|\\frac{X^tX}{n}-Id\\right|\\leq \\frac{1}{14k}.\\hspace{2cm} (1) $$\n",
    "\n",
    "**$X^tX$ vérifie-t-elle cette hypothèse? Que vaut $\\left|\\frac{X^tX}{n}-Id\\right|_\\infty$ pour notre $X$?**\n",
    "\n",
    "*Ici, $|A|_\\infty$ désigne le plus grand coefficient en valeur absolue de la matrice $A$.*\n",
    "\n",
    "On pourra utiliser les méthodes\n",
    "\n",
    "    np.amax\n",
    "    np.identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(np.linalg.amax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a généré les coefficients de notre matrice $X$ (pour un $n$ fixé) à l'aide d'un tirage aléatoire suivant une variable aléatoire de loi $\\mathcal{N}(0,1)$.\n",
    "On note $\\mathbb{X}_{i,j}^n$ les coefficients de $\\mathbb{X}=X^tX$; chacun de ces coefficients est alors la somme de variables aléatoires indépendantes:\n",
    "$$\\mathbb{X}_{i,j}^n=\\sum_{l=1}^n X_{l,i}X_{l,j}.$$\n",
    "\n",
    "**Pourquoi $\\frac{1}{n}\\mathbb{X}^n$ converge vers $Id$ lorsque $n\\rightarrow\\infty$?**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On peut aussi utiliser le théorème central limite pour estimer la distance entre $\\mathbb{X}_{i,j}$ et sa limite quand $n\\rightarrow \\infty$. On obtient alors un intervalle de confiance pour chaque $\\frac{1}{n}\\mathbb{X}_{i,j}$ autour de $\\mathbf{1}_{\\{i=j\\}}$.\n",
    "\n",
    "**Donner un intervalle de confiance de niveau $\\alpha=0.05$ de $\\frac{1}{n}\\mathbb{X}_{i,j}^n$ autour de sa limite.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut même calculer $\\mathbb{E}\\left(\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}\\right)^2$ pour avoir une idée de la valeur moyenne des coefficients de $\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}$.\n",
    "\n",
    "**Calculer $\\mathbb{E}\\left(\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}\\right)^2$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cet intervalle de confiance nous indique donc, dans l'idée de vérifier l'équation $(1)$, que chaque entrée de la matrice $\\frac{X^tX}{n}-Id$ est plus petite, en valeur absolue, que $\\approx 0.14$ - puisque $n=200$, avec probabilité $0.95$.\n",
    "\n",
    "Et si on regarde la valeur moyenne de $|\\frac{1}{n}\\mathbb{X}_{i,j}-\\mathbf{1}_{i=j}|$, on obtient \n",
    "$$\n",
    "\\mathbb{E}\\left|\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}\\right|\\leq \\left(\\mathbb{E}\\left(\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}\\right)^2\\right)^{1/2} = \\frac{1}{\\sqrt{n}} \\approx 0.07\n",
    "$$\n",
    "\n",
    "On n'est pas encore à $\\frac{1}{14k}\\approx 0.007$ qu'il faudrait pour assurer la condition d'incohérence $(1)$.\n",
    "\n",
    "En fait, les calculs que l'on fait sont valables pour chaque $\\frac{1}{n}\\mathbb{X}_{i,j}^n$, mais la condition $(1)$ demande à ce que le *maximum* des coefficients de $|\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}|$ soit majoré, ce qui est plus difficile à obtenir (i.e. plus rare, moins probable) que de demander qu'un des coefficients soit majoré.\n",
    "\n",
    "Mais ça nous permet de comprendre la chose suivante:\n",
    "\n",
    "*puisqu'en moyenne $\\left|\\frac{1}{n}\\mathbb{X}_{i,j}^n-\\mathbf{1}_{i=j}\\right|\\approx \\frac{1}{\\sqrt{n}}$ et que l'on veut satisfaire $(1)$, il faudrait au moins $k\\lesssim\\sqrt{n}$*\n",
    "\n",
    "En fait, il existe une autre version de ce théorème qui affirme que l'on peut obtenir la même majoration (avec une grande probabilité):\n",
    "\n",
    "$$\\mathbb{E}\\frac{1}{n}\\|Y-X\\hat\\beta_{LASSO}\\|^2\\lesssim k\\sigma^2\\frac{\\log(2d)}{n}, \\hspace{2cm}(2)$$\n",
    "où ici $k=len(\\beta)=10$\n",
    "\n",
    "dès que\n",
    "$$k\\log(d)\\lesssim n$$\n",
    "\n",
    "lorsque les coefficients de $X$ sont des variables aléatoires gaussiennes i.i.d.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreur quadratique moyenne\n",
    "\n",
    "On va essayer de vérifier l'équation (2). Pour ça, on se fixe d'abord $d=7000$,\n",
    "puis on calcule $\\frac{1}{n}\\|Y-X\\hat\\beta_{LASSO}\\|^2$ pour différentes valeurs de $k$ et de $n$.\n",
    "\n",
    "Pour ça, on crée plusieurs vecteurs $\\beta$, que l'on range dans une grande matrice $\\rm B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=700\n",
    "n=200\n",
    "kr=100\n",
    "m=5\n",
    "\n",
    "B=np.zeros((d,1))\n",
    "for k in range(1,kr):\n",
    "    B=np.concatenate((B,np.concatenate((m*np.ones((k,1)),np.zeros((d-k,1))))),axis=1)\n",
    "B.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on génère $Y$ en fonction de $X$ pour chaque ligne de $\\rm B$.\n",
    "\n",
    "**Générer ces $Y$ en fonction des lignes de $\\rm B$ et calculer à chaque fois $\\frac{1}{n}\\|Y-X\\hat\\beta_{LASSO}\\|^2$, pour $k=1...100$, $n=200$ et $d=700$.\n",
    "Afficher le résultat sur un graphe et comparer à $\\frac{1}{n}\\|\\beta - \\hat\\beta_{LASSO}\\|^2$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Faire varier $m=5$ et interpréter les différences entre les graphes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode avec pénalisation $|\\hspace{0.3cm}|_0$.\n",
    "\n",
    "Nous avons vu qu'il est possible de pénaliser la minimisation de $\\beta \\mapsto \\|Y-X\\beta\\|$ par $+\\lambda |\\beta|_0$ pour chercher les coordonnées non nulles de $\\beta$.\n",
    "Ou de manière équivalente, on peut minimiser $\\beta \\mapsto \\|Y-X\\beta\\|$ sous la contrainte $|\\beta|_0\\leq k$.\n",
    "\n",
    "**Implémenter cette minimisation pour $k=3$**\n",
    "\n",
    "On pourra utiliser \n",
    "\n",
    "    np.inf\n",
    "    np.argmin\n",
    "    np.unravel_index\n",
    "    \n",
    "**Puis tester pour $d=20,50,100$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode Ridge\n",
    "\n",
    "**Implémenter la méthode Ridge à l'aide des formules du cours.**\n",
    "**Appliquer pour $X$ et $\\beta$ définis par:**\n",
    "\n",
    "    X=np.random.rand(n,d-1)\n",
    "    X=np.concatenate((np.transpose([2*X[:,0]]),X),axis=1)\n",
    "    \n",
    "    beta=np.concatenate(([1],np.zeros(d-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On ne retrouve pas le $\\beta$ d'origine. Pourquoi?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode Elastic Net\n",
    "\n",
    "La méthode Elastic Net combine les deux pénalisations $\\| . \\|_1$ et $\\| . \\|_2$.\n",
    "On minimise alors:\n",
    "\n",
    "$$ \\beta \\mapsto \\|Y-X\\beta\\|_2^2 + \\alpha\\left(\\lambda \\|\\beta\\|_1 + (1-\\lambda)\\|\\beta\\|_2^2\\right) $$\n",
    "---------------\n",
    "\n",
    "dans l'espoir de combiner les deux effets des méthodes LASSO et Ridge.\n",
    "\n",
    "Cette méthode est codée dans la classe\n",
    "\n",
    "    linear_model.ElasticNet\n",
    "    \n",
    "**Implémenter cette méthode pour un jeu de données simulé où:**\n",
    "+ $X$ est généré par des variables aléatoires de loi $\\mathcal{N}(0,1)$ i.i.d. pour ses $d-3$ premières colonnes, puis les 3 dernières sont des combinaisons linéaires de deux des colonnes précédentes.\n",
    "+ $\\varepsilon$ est une suite de v.a.i.i.d. de loi $\\mathcal{N}(0,1)$.\n",
    "+ $\\beta=(1,-3,0,...,0,0,3)$.\n",
    "\n",
    "On demande de retourner $\\hat\\beta$ par cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commenter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
