{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression non linéaire\n",
    "===================\n",
    "\n",
    "L'objectif de ce TP est d'effectuer les différents calculs des notions vues en cours sur le modèle\n",
    "$$Y = f(X) + \\varepsilon$$\n",
    "==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les librairies dont on aura besoin pour le TP, à savoir *numpy* pour les outils mathématiques et *sklearn* pour les outils d'apprentissage.\n",
    "\n",
    "Pour que les *plot* s'affiche dans la page, on ajoute\n",
    "\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Méthode des moindres carrés ordinaire, avec une base orthnormée\n",
    " \n",
    " On se place dans le contexte de régression sur une base orthonormée de $L_2([0;1])$ et dans le cas où le fonction $f$ est dans  \n",
    " $$ H(\\beta,L)=\\{f\\in L^2([0;1]) | f^{(\\beta-1)} <<\\lambda, \\int f^{(\\beta)}d\\lambda\\leq L\\}.$$\n",
    " \n",
    " On pose pour tout $k\\in\\mathbb{N}$, et tout $x\\in [0;1]$\n",
    "\n",
    " \\begin{align}\n",
    " \\phi_1&=1\\\\\n",
    " \\phi_{2k}(x)&=\\sqrt{2}\\cos(2\\pi k x)\\\\\n",
    " \\phi_{2k+1}(x)&=\\sqrt{2}\\sin(2\\pi k x)\\\\\n",
    " \\end{align}\n",
    " \n",
    " **Compléter le définition de phi suivante**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_odd(k):\n",
    "    if k==2*np.trunc(k/2):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def phi(k,x):\n",
    "    if x<0 or x>1:\n",
    "        return 0\n",
    "    elif k==1:\n",
    "        return 1\n",
    "    elif is_odd(k):\n",
    "        return  \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De la même façon, définir les fonctions \n",
    "$$f_1:x\\mapsto \\mathbf{1}_{\\{0\\leq x\\leq 1/2\\}} 2*x + \\mathbf{1}_{\\{1/2<x\\leq 1\\}}\\log(x)$$\n",
    "et \n",
    "$$f_2:x\\mapsto \\mathbf{1}_{\\{0\\leq x \\leq 1\\}} \\left(\\frac{1}{2} - \\left|x-\\frac{1}{2}\\right|\\right).$$\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "\n",
    "    return x\n",
    "\n",
    "def fdeux(x):\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace le graphe de ces deux fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p=1000\n",
    "fun_plot=[]\n",
    "for x in np.arange(0,1,1.0/p):\n",
    "    fun_plot.append(fun(x))\n",
    "plt.plot(np.arange(0,1,1.0/p),fun_plot,'b.')\n",
    "fdeux_plot=[]\n",
    "for x in np.arange(0,1,1.0/p):\n",
    "    fdeux_plot.append(fdeux(x))\n",
    "plt.plot(np.arange(0,1,1.0/p),fdeux_plot, 'g--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On génère alors nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "eps=0.1*np.random.normal(0,1,n)\n",
    "Y=np.zeros(n,dtype=float)\n",
    "for x in range(n):\n",
    "    Y[x]=fun(float(x)/n)+eps[x]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "print(Y.shape)\n",
    "print(np.arange(0,1,1.0/n).shape)\n",
    "ax.scatter(np.arange(0,1,1.0/n),Y, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(r'$Y$ en fonction de $X$')\n",
    "\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant générer les $\\phi_j(x_i)$ sur lesquels on appliquera la régression LASSO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m=n-1\n",
    "phiX=np.zeros((n,m),dtype=np.float)\n",
    "for x in range(n):\n",
    "    for j in range(m):\n",
    "        phiX[x,j]=phi(j+1,float(x)/n)\n",
    "        \n",
    "print(phiX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utiliser la méthode LASSO pour déterminer un estimateur de $f$, avec $m=n-1$, pour $f_1$ et $f_2$.**\n",
    "**Faire varier le coefficient $\\lambda$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l=0.2*np.sqrt(np.log(2*m)/n)\n",
    "print l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracer sur un même graphe, $f_1$ et son estimateur $\\hat f_1$. De même pour $f_2$ et son estimateur $\\hat f_2$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculer l'erreur de l'estimateur: $\\frac{1}{n}\\sum_{i=1}^n|\\hat f(x_i) - f(x_i)|^2$ pour chacune des fonctions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracer cette erreur en fonction de $n$ pour $n=10..1000$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche maintenant à utiliser l'estimateur des $k$ plus proches voisins.\n",
    "Pour celà on pourra utiliser\n",
    "\n",
    "    KNeighborsRegressor(n_neighbors=k)\n",
    "    neigh.fit(X, Y)\n",
    "    \n",
    "**Estimer $f_1$ et $f_2$ à l'aide des $k$-plus proches voisins.\n",
    "Afficher le graphe des fonctions et leur estimateur.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=int(np.floor(np.power(n,2.0/3.0))+1)\n",
    "X=np.arange(0,1,1.0/n)\n",
    "X=X.reshape(-1,1)\n",
    "print k\n",
    "neigh = KNeighborsRegressor(n_neighbors=k)\n",
    "#à partir du cours: alpha=1, d=1 donc k=n^{2/3}\n",
    "neigh.fit(X, Y)\n",
    "\n",
    "hatf=[]\n",
    "Xplot=np.arange(0,1,1.0/p)\n",
    "Xplot=Xplot.reshape(-1,1)\n",
    "for x in Xplot:\n",
    "    hatf.append(neigh.predict([x]))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(np.arange(0,1,1.0/p),hatf, alpha=0.5)\n",
    "ax.scatter(np.arange(0,1,1.0/p),fun_plot, alpha=0.5, color='red')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(r'$Y$ en fonction de $X$')\n",
    "\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On introduit les estimateurs à noyaux.\n",
    "L'estimateur de Nadaraya-Watson pour un noyau $K$ est donnée par\n",
    "$$ \\hat f(x)=\\sum_{i=1}^n w_{i,n}(x)Y_i.$$\n",
    "où\n",
    "$$ w_{i,n}(x)=\\frac{K\\left(\\frac{x-X_i}{h}\\right)}{\\sum_{i=1}^n K\\left(\\frac{x-X_i}{h}\\right)}.$$\n",
    "\n",
    "$K$ permet de mettre un poids sur chaque $Y_i$ qui dépend de la proximité de $x$ à $X_i$, qui est paramétré par $h>0$.\n",
    "\n",
    "On peut montrer que si $R=\\int K^2(u)du<\\infty$ et $K_2=\\int K(u)u^2du<\\infty$ alors\n",
    "$$EQM(\\hat f(x))\\lesssim h^4 + \\frac{1}{nh},$$\n",
    "si la densité de $X$ est minorée et $f$ est deux fois différentiable.\n",
    "\n",
    "**Calculer la vitesse de convergence de l'EQM pour un choix de $h$ optimal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit\n",
    "$$K:u\\mapsto e^{-\\frac{u^2}{2}}.$$\n",
    "\n",
    "**Utiliser ce noyau pour calculer l'estimateur de Nadaray-Watson de $f_1$ et $f_2$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
