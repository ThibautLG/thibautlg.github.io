{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce TP est d'utiliser deux méthodes différentes sur des données simulées et de voir l'influence de leur paramètres sur leur résultat.\n",
    "\n",
    "On commence par les SVMs.\n",
    "\n",
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC     # \"Support Vector Classifier\"\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas séparable\n",
    "\n",
    "Le premier cas que auquel on s'intéresse est celui où les données sont linéairement séparables. C'est à dire, elles sont générées par\n",
    "$$ Y = f(X) $$\n",
    "=======\n",
    "où $f$ est le signe d'une fonction affine.\n",
    "Commençons par générer et afficher ces données.\n",
    "\n",
    "##### Exemple 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=150\n",
    "X=np.random.randn(n,2)\n",
    "w=[1,-1]\n",
    "b=0.5\n",
    "Y=np.sign((np.dot(X,w)+0.5)).ravel()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"spring\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici les données sont séparables par une droite. Le module SVC de *scikitlearn* permet de trouver la droite qui passe avec la marge maximale séparant les données. On cherche alors à résoudre le problème suivant:\n",
    "$$ \\min \\left\\{ \\left. \\frac{1}{2} \\|w\\|^2 \\right| y_i(<w,x_i>+b)\\geq 1, \\forall 1\\leq i \\leq n \\right\\}$$\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear',C=np.Inf)\n",
    "clf.fit(X, Y)\n",
    "print clf.coef_\n",
    "print clf.intercept_\n",
    "print clf.coef_/np.linalg.norm(clf.coef_)\n",
    "print w/np.linalg.norm(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quel est le lien entre** *clf.intercept_* ** et la formulation mathématique du problème?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compléter la fonction** *plot_svc* **suivante afin qu'elle trace la droite portée par $\\beta$ et celle trouvée par SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_svc(clf, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    wstar=clf.coef_.ravel()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    ystar = \n",
    "    y = \n",
    "    ax.plot(ystar,x,c=\"red\")\n",
    "    ax.plot(y,x,c=\"green\")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"spring\");\n",
    "plot_svc(clf,plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas non séparable\n",
    "\n",
    "On considère maintenant le cas non séparable; certaines données ne sont pas bien étiquetées pour une séparatation linéaire. On autorise alors quelques erreurs dans la recherche de la marge maximale:\n",
    "$$ \\min \\left\\{ \\left. \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\right| y_i(<w,x_i>+b)\\geq 1 - \\xi_i, \\forall 1\\leq i \\leq n \\right\\}$$\n",
    "-----\n",
    "\n",
    "Cette minimisation est implémentée dans la module SVC; c'est la raison pour laquelle on a posé *C=np.Inf* plus haut.\n",
    "On intègre un bruit dans la génération de nos données pour des avoir des données non séparables.\n",
    "\n",
    "##### Exemple 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=150\n",
    "X=np.random.randn(n,2)\n",
    "w=[1,-1]\n",
    "b=0.5\n",
    "eps=0.7*np.random.randn(n,1)\n",
    "Y=np.sign((np.dot(X,w)+0.5).reshape(-1,1)+eps).ravel()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"spring\");\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "plot_svc(clf,plt.gca());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il possible de voir les points pour lesquels la contraintes est active; ce sont les **vecteurs support**. On les retrouve à l'aide de l'attribut \n",
    "\n",
    "    support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='spring')\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=50, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À l'aide de la fonction ``interact`` d'iPython, on peut voir l'effet de la constante *C* sur la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "\n",
    "n=150\n",
    "X=np.random.randn(n,2)\n",
    "w=[1,-1]\n",
    "b=0.5\n",
    "eps=np.random.randn(n,1)\n",
    "Y=np.sign((np.dot(X,w)+0.5).reshape(-1,1)+eps).ravel()\n",
    "\n",
    "\n",
    "def call_plot_svc(C=1):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"spring\");\n",
    "    clf = SVC(kernel='linear',C=C/100.)\n",
    "    clf.fit(X, Y)\n",
    "    plot_svc(clf,plt.gca());\n",
    "\n",
    "    \n",
    "interact(call_plot_svc, C=[1, 50], kernel='linear');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi générer les données de la manière suivante.\n",
    "On tire une variable gaussienne centrée en $(0,2)$ puis une centrée en $(-2,0)$.\n",
    "\n",
    "L'objectif est alors de retrouver suivant quelle loi a été tirée la variable aléatoire.\n",
    "\n",
    "##### Exemple 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=150\n",
    "X=np.append(np.random.randn(n,2)+[0,2],np.random.randn(n,2)+[-2,0],axis=0)\n",
    "w=[1,1]\n",
    "b=0\n",
    "Y=np.append(np.zeros(n),np.ones(n))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"spring\");\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, Y)\n",
    "plot_svc(clf,plt.gca());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'est pas possible de faire mieux que de scinder l'espace en deux pour retrouver l'origine de la variable alétoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM à noyaux\n",
    "\n",
    "Les SVMs à noyaux permettent de rendre *linéairement* séparables des données qui ne le sont pas, en les plongeant dans un espace de plus grande dimension.\n",
    "\n",
    "Cette fois, on définit deux variables aléatoires gaussiennes centrées en $(0,0)$ de variance $1$ et $16$.\n",
    "\n",
    "L'objectif est encore de retrouver la loi de chacune des variables aléatoires.\n",
    "\n",
    "##### Exemple 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=150\n",
    "X=np.append(np.random.randn(n,2),4*np.random.randn(n,2),axis=0)\n",
    "print X.shape\n",
    "w=[1,1]\n",
    "b=0\n",
    "Y=np.append(np.zeros(n),np.ones(n),axis=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"spring\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_color_decision(clf, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    h=0.05\n",
    "    # Create color maps\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=Y, cmap=cmap_bold)\n",
    "    # Plot also the training points\n",
    "    ax.xlim(xx.min(), xx.max())\n",
    "    ax.ylim(yy.min(), yy.max())\n",
    "    ax.title(\"SVM classification\")\n",
    "    ax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(X, Y)\n",
    "plot_color_decision(clf,plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour contourner ce problème, on va enoyer les données dans un espace dans lequel elles seront linéairement séparable. On peut faire ça dans un espace de dimension infini (par exemple $L_2$), mais on pour comprendre le principe, on va voir comment on peut envoyer les données dans un espace en 3D. Pour ça on ajoute une troisème coordonnée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=Y, s=50, cmap='spring')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[-90, 90], azip=(-180, 180));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est plus raisonnable ici de vouloir séparer les données par un plan linéaire dans ce plongement en 3D!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le noyau gaussien (issu d'un plongement des données dans un espace de dimension infinie) donne le résultat suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X, Y)\n",
    "plot_color_decision(clf,plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utiliser la fonction**\n",
    "\n",
    "    interact\n",
    "    \n",
    "**pour voir l'influence du paramètre** *gamma* **sur le méthode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K plus proches voisins\n",
    "\n",
    "**Reprenez les 4 exemples de la méthode SVM et implémentez le méthode des k-plus proches voisins à l'aide de la classe **\n",
    "\n",
    "    neighbors.KNeighborsClassifier   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "help(neighbors.KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
