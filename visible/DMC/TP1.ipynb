{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Régression linéaire\n",
    "===================\n",
    "\n",
    "L'objectif de ce TP est d'effectuer les différents calculs des notions vues en cours sur le modèle\n",
    "$$Y = X\\beta + \\varepsilon$$\n",
    "===========================\n",
    "dans le cas où $X$ est ou non de plein rang."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les librairies dont on aura besoin pour le TP, à savoir *numpy* pour les outils mathématiques et *sklearn* pour les outils d'apprentissage (LASSO, Ridge, Elastic net).\n",
    "\n",
    "Pour que les *plot* s'affichent dans la page, on ajoute\n",
    "\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Méthode des moindres carrés ordinaire\n",
    " \n",
    " Dans un premier temps, nous allons générer $Y$ en fonction de $X$ à partir d'un $\\beta$ que l'on se fixe.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=50\n",
    "beta=[1,4,1,-7,-2,1,0.7,0.1,3,0.01]\n",
    "X=np.random.randn(n,10)\n",
    "eps=np.random.normal(0,1,n)\n",
    "Y=np.dot(X,beta)+eps\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est de retrouver $\\beta$ à partir de l'observation de $X$ et $Y$.\n",
    "\n",
    "Dans un premier, on affiche sur un graphe $Y$ en fonction d'une coordonnée de $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y, X[:,8], alpha=0.5)\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title(r'$Y$ en fonction de $X_9\\beta_9$')\n",
    "\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va avoir du mal à prédire $Y$ seulement à partir de $X_9$.\n",
    "On calcule l'erreur quadratique moyenne dans cette régression de $Y$ sur $X_9$ lorsque $\\beta_9=3$ est connu.\n",
    "\n",
    "Pour cela, on utilise\n",
    "\n",
    "    np.linalg.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(np.linalg.norm)\n",
    "r1=np.linalg.norm(Y-3*X[:,8])/n\n",
    "r2=np.linalg.norm(Y-np.dot(X,beta))/n\n",
    "print([r1,r2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculer l'erreur quadratique moyenne de la régression de $Y$ sur $X$ lorsque $\\beta$ est connu.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme attendu, l'écart quadratique moyen est inférieur lorsque l'on régresse $Y$ sur toute les colonnes de $X$.\n",
    "Autrement dit, en utilisant toute l'information que l'on a sur $Y$ (i.e. toute les colonnes de $X$), on fait mieux.\n",
    "\n",
    "Maintenant, on suppose que $\\beta$ est inconnue. \n",
    "On veut le retrouver à partir de nos observations de $X$ et $Y$.\n",
    "\n",
    "**Estimer $\\beta$ en appliquant la méthode des moindres carrés.**\n",
    "\n",
    "On pourra utiliser les méthodes\n",
    "\n",
    "    np.linalg.inv\n",
    "    np.linalg.dot\n",
    "    np.transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(np.linalg.inv)\n",
    "help(np.linalg.dot)\n",
    "help(np.transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variance de $\\varepsilon$ est $1$. À partir de la formule du cours, **donner la variance des $\\hat\\beta_i$ (conditionnellement à $X$).**\n",
    "\n",
    "On pourra utiliser \n",
    "\n",
    "        np.diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(np.diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En déduire un intervalle de confiance à 0.95 des $\\beta_j$.\n",
    "Les *vrais* $\\beta_j$ sont-ils dans cet intervalle?**\n",
    "\n",
    "Pour rappel, le quantile d'ordre 0.975 de la loi normale est $\\approx$ 1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cas parcimonieux\n",
    "\n",
    "On reprend maintenant le même contexte, mais cette fois, des colonnes sont ajoutées à $X$. Autrement dit, l'information qui nous permet d'estimer $X$ (i.e. les colonnes $j$ de $X$ ayant un grand coefficient $\\beta_j$) est masquée parmi un grand nombre de colonnnes de $X$.\n",
    "On note alors $\\beta'$ ce nouveau vecteur.\n",
    "\n",
    "*Pour les modèles avec parcimonie, on notera $k$ le nombre de coordonnées non nulles de $\\beta$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=7000\n",
    "k=len(beta)\n",
    "betap=np.concatenate((beta,np.zeros(d-k)))\n",
    "np.random.shuffle(betap)\n",
    "print(betap)\n",
    "Xp=np.random.randn(n,d)\n",
    "Yp=np.dot(Xp,betap)+eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que renvoie la méthode des moindres carrés?\n",
    "Commenter**\n",
    "\n",
    "*Enregistrer votre travail avant d'executer la cellule suivante*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hatbetap=np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(Xp),Xp)),np.transpose(Xp)),Y)\n",
    "print(hatbetap)\n",
    "hatbetap-betap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir retrouver nos coefficients, on va utiliser la méthode LASSO dans un premier temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode LASSO\n",
    "\n",
    "La méthode de régression LASSO est implémentée dans la librairie *scikitlearn* via la classe\n",
    "\n",
    "    linear_model.Lasso\n",
    "   \n",
    "Nous avons vu qu'il est possible d'ajouter une colonne de $1$ à $X$ pour des données non centrées. Cette classe permet d'automatiser l'ajout de cette colonne, qui est alors appelée *intercept*.\n",
    "Comme on sait que dans notre modèle, il n'y a pas de colonne de 1 et que cette classe l'ajoute par défaut, on va lui demander de ne pas l'ajouter via l'argument *intercept=False*\n",
    "\n",
    "*Dans cette classe, le $\\lambda$ du cours est appelé* alpha\n",
    "\n",
    "On utilise la valeur $\\lambda=8\\sqrt{\\frac{2d}{n}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l=8*np.sqrt(np.log(2*d)/n)\n",
    "clf = linear_model.Lasso(alpha=l,fit_intercept=False)\n",
    "clf.fit(Xp, Yp)                                        #effecture les calculs pour estimer beta\n",
    "hatbetap=clf.coef_                                     #renvoie les coefficients estimés par la méthode LASSO\n",
    "\n",
    "print(betap[betap!=0])\n",
    "print(betap[hatbetap!=0])\n",
    "print(hatbetap[betap!=0])\n",
    "print(np.linalg.norm(Yp-np.dot(Xp,hatbetap))/n,10*np.log(2*d)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commenter. Est-ce que la méthode semble bien fonctionner?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode avec pénalisation $|\\hspace{0.3cm}|_0$.\n",
    "\n",
    "Nous avons vu qu'il est possible de pénaliser la minimisation de $\\beta \\mapsto \\|Y-X\\beta\\|$ par $+\\lambda |\\beta|_0$ pour chercher les coordonnées non nulles de $\\beta$.\n",
    "Ou de manière équivalente, on peut minimiser $\\beta \\mapsto \\|Y-X\\beta\\|$ sous la contrainte $|\\beta|_0\\leq k$.\n",
    "\n",
    "**Implémenter cette minimisation pour $k=3$**\n",
    "\n",
    "On pourra utiliser \n",
    "\n",
    "    np.inf\n",
    "    np.argmin\n",
    "    np.unravel_index\n",
    "    \n",
    "**Puis tester pour $d=20,50,100$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=20\n",
    "X=np.random.randn(n,d)\n",
    "eps=np.random.normal(0,1,n)\n",
    "beta=np.concatenate((np.zeros(d-3),[3,2,4]))\n",
    "\n",
    "Y=np.dot(X,beta)+eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode Ridge\n",
    "\n",
    "**Implémenter la méthode Ridge à l'aide des formules du cours.**\n",
    "**Appliquer pour $X$ et $\\beta$ définis par:**\n",
    "\n",
    "    X=np.random.rand(n,d-1)\n",
    "    X=np.concatenate((np.transpose([2*X[:,0]]),X),axis=1)\n",
    "    \n",
    "    beta=np.concatenate(([1],np.zeros(d-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=4\n",
    "n=5\n",
    "X=np.random.rand(n,d-1)\n",
    "\n",
    "X=np.concatenate((np.transpose([2*X[:,0]]),X),axis=1)\n",
    "eps=np.random.normal(0,1,n)\n",
    "beta=np.concatenate(([1],np.zeros(d-1)))\n",
    "\n",
    "Y=np.dot(X,beta)+eps\n",
    "l=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On ne retrouve pas le $\\beta$ d'origine. Pourquoi?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode Elastic Net\n",
    "\n",
    "La méthode Elastic Net combine les deux pénalisations $\\| . \\|_1$ et $\\| . \\|_2$.\n",
    "On minimise alors:\n",
    "\n",
    "$$ \\beta \\mapsto \\|Y-X\\beta\\|_2^2 + \\alpha\\left(\\lambda \\|\\beta\\|_1 + (1-\\lambda)\\|\\beta\\|_2^2\\right) $$\n",
    "---------------\n",
    "\n",
    "dans l'espoir de combiner les deux effets des méthodes LASSO et Ridge.\n",
    "\n",
    "Cette méthode est codée dans la classe\n",
    "\n",
    "    linear_model.ElasticNet\n",
    "    \n",
    "**Implémenter cette méthode pour un jeu de données simulé où:**\n",
    "+ $X$ est généré par des variables aléatoires de loi $\\mathcal{N}(0,1)$ i.i.d. pour ses $d-3$ premières colonnes, puis les 3 dernières sont des combinaisons linéaires de deux des colonnes précédentes.\n",
    "+ $\\varepsilon$ est une suite de v.a.i.i.d. de loi $\\mathcal{N}(0,1)$.\n",
    "+ $\\beta=(1,-3,0,...,0,0,3)$.\n",
    "\n",
    "On demande de retourner $\\hat\\beta$ par cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commenter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
